{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生态科学数据中心"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取文件\n",
    "import json\n",
    "\n",
    "with open('C:\\\\Users\\\\jsk94\\\\Desktop\\\\result.txt', 'r', encoding='utf-8') as f:\n",
    "    all_metadata = []\n",
    "    for line in f.readlines():\n",
    "        dic = json.loads(line)\n",
    "        all_metadata.append(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_score=[]\n",
    "consistency_score=[]\n",
    "accuracy_score=[]\n",
    "timeline_score=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9230769230769231, 0.9230769230769231, 0.9230769230769231, 0.9230769230769231, 0.9230769230769231, 0.9230769230769231, 0.9230769230769231, 0.9230769230769231, 0.9230769230769231, 0.9230769230769231, 0.9230769230769231, 0.9230769230769231, 0.9230769230769231, 0.9230769230769231, 0.9230769230769231, 0.9230769230769231, 0.9230769230769231, 0.9230769230769231, 0.9230769230769231, 0.9230769230769231]\n"
     ]
    }
   ],
   "source": [
    "#完整性计算\n",
    "for metadata in all_metadata:\n",
    "    complete = 0\n",
    "    count_comple = 0\n",
    "    for key,value in metadata.items():\n",
    "    #    print(key,value)\n",
    "        count_comple += 1\n",
    "        if value != None:\n",
    "            complete += 1\n",
    "    complete_score.append(complete/count_comple)\n",
    "print(complete_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.9, 0.9, 0.96, 0.96, 0.9, 0.9, 0.9, 0.96, 0.96, 0.9, 0.96, 0.96, 0.9]\n"
     ]
    }
   ],
   "source": [
    "#时效性计算\n",
    "import datetime\n",
    "\n",
    "for metadata in all_metadata:\n",
    "    for key,value in metadata.items():\n",
    "        if key == \"实体最近更新日期\":\n",
    "            time = int(value[0:4])\n",
    "            current = datetime.datetime.now().year\n",
    "            timeline =1- (current-time)/50\n",
    "    timeline_score.append(timeline)\n",
    "print(timeline_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 0.8311261070726224, 0.8308954947606277, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8294572134479452, 1.0, 0.8271958298033916, 0.8302680942625847, 1.0, 0.8272872402427887, 0.8301083374237219]\n"
     ]
    }
   ],
   "source": [
    "#准确性计算 对元数据字符长度取e为底的对数\n",
    "import pycorrector\n",
    "import math\n",
    "\n",
    "#目前存在错误：摘要：采样重复数祥见监测规范。\n",
    "for metadata in all_metadata:\n",
    "    #记录错别字的个数\n",
    "    count_accu = 0\n",
    "    #文本字符个数\n",
    "    length = 0\n",
    "    for key,value in metadata.items():\n",
    "        if value != None:\n",
    "            corrected_sent, detail = pycorrector.correct(value)\n",
    "            count_accu += len(detail)\n",
    "            length += len(value)\n",
    "    accuracy_score.append(1-(count_accu-1)/math.log(length))\n",
    "print(accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334]\n"
     ]
    }
   ],
   "source": [
    "#一致性计算 核验代码、共享方式、实体最近更新日期、数据库类型、数据量、记录总数的格式规范\n",
    "import re\n",
    "\n",
    "for metadata in all_metadata:\n",
    "    #考察的字段的总数\n",
    "    count_consis = 6\n",
    "    #合格的字段的数量\n",
    "    consis = 0\n",
    "    #共享方式\n",
    "    share=[ \"完全共享\",\"限制共享\",\"协议共享\"]\n",
    "    for key,value in metadata.items():\n",
    "        if key == \"代码\":\n",
    "            pattern = re.compile(\"\\w*\")\n",
    "            if re.match(pattern,value) != None:\n",
    "                consis += 1\n",
    "        if key == \"共享方式\":\n",
    "            if value in share:\n",
    "                consis += 1\n",
    "        if key == \"实体最近更新日期\":\n",
    "            pattern = re.compile(\"\\d{4}-\\d{2}-\\d{2}\")\n",
    "            if re.match(pattern,value) != None:\n",
    "                consis += 1\n",
    "        if key ==\"数据库类型\":\n",
    "            pattern = re.compile(\"\\w*\")\n",
    "            if re.match(pattern,value) != None:\n",
    "                consis += 1\n",
    "        if key == \"数据量\":\n",
    "            pattern = re.compile(\"\\d*\")\n",
    "            if re.match(pattern,value) != None:\n",
    "                consis += 1      \n",
    "        if key == \"记录总数\":\n",
    "            pattern = re.compile(\"\\d*\")\n",
    "            if re.match(pattern,value) != None:\n",
    "                consis += 1\n",
    "    consistency_score.append(consis/count_consis)\n",
    "print(consistency_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9141025641025641, 0.9291025641025641, 0.9291025641025641, 0.9291025641025641, 0.9291025641025641, 0.8868840908707196, 0.8868264377927211, 0.9141025641025641, 0.9141025641025641, 0.9291025641025641, 0.9291025641025641, 0.9141025641025641, 0.9141025641025641, 0.8714668674645504, 0.9291025641025641, 0.885901521553412, 0.8716695876682102, 0.9291025641025641, 0.8859243741632612, 0.8716296484584946]\n"
     ]
    }
   ],
   "source": [
    "average=[]\n",
    "for i in range(20):\n",
    "    mean = (complete_score[i]+consistency_score[i]+accuracy_score[i]+timeline_score[i])/4\n",
    "    average.append(mean)\n",
    "print(average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9403169158506841\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "for i in accuracy_score:\n",
    "    sum += i\n",
    "mean = sum/20\n",
    "print(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9360000000000002\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "for i in timeline_score:\n",
    "    sum += i\n",
    "mean = sum/20\n",
    "print(mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 农业科学数据中心"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算完整性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666]\n"
     ]
    }
   ],
   "source": [
    "for item in records:\n",
    "    field_count = len(item)\n",
    "    comple_count = 0\n",
    "    for v in item.values():\n",
    "        if v != '' and v != None:\n",
    "            comple_count +=1\n",
    "    temp = comple_count/field_count\n",
    "    complete.append(temp)\n",
    "print(complete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 时效性计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7, 0.74, 0.7, 0.74, 0.74, 0.74, 0.74, 0.76, 0.76, 0.74, 0.6599999999999999, 0.6599999999999999, 0.6599999999999999, 0.6599999999999999, 0.6599999999999999, 0.6599999999999999, 0.6599999999999999, 0.6599999999999999, 0.6599999999999999, 0.6599999999999999, 0.96, 0.96, 0.96, 0.72, 0.7, 0.7, 0.7, 0.7, 0.92, 0.92, 0.76, 0.74, 0.64, 0.6799999999999999, 0.6799999999999999, 0.6799999999999999, 0.6799999999999999, 0.7, 0.76, 0.7, 0.62, 0.8200000000000001, 0.8200000000000001, 0.74, 0.8200000000000001, 0.8200000000000001, 0.74, 0.8200000000000001, 0.8200000000000001, 0.84]\n"
     ]
    }
   ],
   "source": [
    "for item in records:\n",
    "    year = int(item['涉及时间'])\n",
    "    current = datetime.datetime.now().year\n",
    "    temp =1- (current-year)/50\n",
    "    timeline.append(temp)\n",
    "print(timeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7416000000000001\n"
     ]
    }
   ],
   "source": [
    "avg = np.asarray(timeline)\n",
    "print(avg.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算文本区分度（相似度）\n",
    "使用tfidf实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "与其他元数据的相似度\n",
      "[0.03533746454180503, 0.05618464287218391, 0.04926465862259573, 0.03533746454180503, 0.026906168601494664, 0.031139480235169128, 0.03198915831174473, 0.04460365734804345, 0.031156527372647305, 0.013616275355904078, 0.014430750389488376, 0.014430750389488376, 0.013296088393853635, 0.049113779043664735, 0.049113779043664735, 0.049113779043664735, 0.051620365405569274, 0.049113779043664735, 0.049113779043664735, 0.0, 0.04696946546473369, 0.027021261891920348, 0.030399531344598045, 0.02064764368518883, 0.02064764368518883, 0.05618464287218391, 0.04801133944063771, 0.05862717796117067, 0.01825361592429025, 0.02040816326530612, 0.0474528497826232, 0.049573522745346536, 0.049573522745346536, 0.06750423947767335, 0.05839018489481235, 0.06604082993117674, 0.05839018489481235, 0.05177616312320591, 0.05177616312320591, 0.05555110588688784, 0.043447522180421014, 0.041242059011354436, 0.04868319225782643, 0.04081996484678619, 0.028632103484504078, 0.04081996484678619, 0.049573522745346536, 0.039811224695675225, 0.02829709436212267, 0.0187346112477232]\n",
      "平均相似度\n",
      "0.039562857188379416\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "from gensim import corpora,models,similarities\n",
    "from collections import defaultdict\n",
    "import pymongo\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "client = pymongo.MongoClient(host='localhost',port = 27017)\n",
    "db = client.spider\n",
    "collection = db.metadata_agridata\n",
    "records = collection.find()\n",
    "\n",
    "uniq = []\n",
    "for index in range(0,50):\n",
    "    copy1 = records.clone()\n",
    "    test = copy1[index]['标题']\n",
    "    #语料库\n",
    "    word_collection = []\n",
    "    text = \"\"\n",
    "    for record in copy1:\n",
    "        text= record['标题']\n",
    "        text_cut = jieba.cut(text)\n",
    "        temp = \"\"\n",
    "        for i in text_cut:\n",
    "            temp += i+' '\n",
    "        word_collection.append(temp)\n",
    "    del word_collection[index]\n",
    "    texts = [[word for word in document.split()] for document in word_collection] \n",
    "    # 去除无意义字符\n",
    "    stop = ['《','》','（','）','、']\n",
    "    texts = [[w for w in text if w not in  stop]for text in texts]\n",
    "#     # 计算词频\n",
    "#     frequency = defaultdict(int)\n",
    "#     for t in texts:\n",
    "#         for w in t:\n",
    "#             frequency[w] += 1\n",
    "    # 建立词典\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    #对语料库进一步处理，得到新语料库\n",
    "    corpus=[dictionary.doc2bow(text)for text in texts]\n",
    "    # 将新语料库通过tf-idf model 进行处理，得到tfidf\n",
    "    tfidf=models.TfidfModel(corpus)\n",
    "    #通过token2id得到特征数\n",
    "    featurenum=len(dictionary.token2id.keys())\n",
    "    #稀疏矩阵相似度，从而建立索引\n",
    "    index=similarities.SparseMatrixSimilarity(tfidf[corpus],num_features=featurenum)\n",
    "\n",
    "    # 加载待测试文本\n",
    "    test_cut = jieba.cut(test)\n",
    "    temp2 = \"\"\n",
    "    for i in test_cut:\n",
    "        if i not in stop:\n",
    "            temp2 += i+\" \" \n",
    "    # 将测试文本通过doc2bow转化为稀疏向量\n",
    "    new_xs = dictionary.doc2bow(temp2.split())\n",
    "    #得到最终相似结果\n",
    "    sim=index[tfidf[new_xs]]\n",
    "    u = sum(sim)/len(word_collection)\n",
    "#     uniq.append(1 - u)\n",
    "    uniq.append(u)\n",
    "print(\"与其他元数据的相似度\")\n",
    "print(uniq)\n",
    "print(\"平均相似度\")\n",
    "print(sum(uniq)/50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9603809512930218\n"
     ]
    }
   ],
   "source": [
    "print(sum(uniq)/50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算一致性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182]\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "import numpy as np\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "client = pymongo.MongoClient(host='localhost',port = 27017)\n",
    "db = client.spider\n",
    "collection = db.metadata_agridata\n",
    "records = collection.find()\n",
    "\n",
    "consistent = [] \n",
    "for record in records:\n",
    "    consis = 9\n",
    "    Id = record['唯一标识号']\n",
    "    pattern = re.compile(\"^[\\dA-Za-z_]$\")\n",
    "    if re.match(pattern,Id) != None:\n",
    "        consis += 1\n",
    "    down = record['下载']\n",
    "    pattern = re.compile(\"[a-zA-z]+://[^\\s]*\")\n",
    "    if re.match(pattern,down) != None:\n",
    "        consis += 1\n",
    "    temp = consis/11\n",
    "    consistent.append(temp)\n",
    "print(consistent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算准确性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7537214541855055, 1.0, 0.778312758675969, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "#准确性计算 对元数据字符长度取e为底的对数\n",
    "import pycorrector\n",
    "import math\n",
    "import pymongo\n",
    "import numpy as np\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "client = pymongo.MongoClient(host='localhost',port = 27017)\n",
    "db = client.spider\n",
    "collection = db.metadata_agridata\n",
    "records = collection.find()\n",
    "\n",
    "accuracy_score = []\n",
    "#目前存在错误：摘要：采样重复数祥见监测规范。\n",
    "for record in records:\n",
    "    text = record['标题'] + record['关键词'] + record['分类'] + record['生产者'] + record['所在机构']\n",
    "    #记录错别字的个数\n",
    "    count_accu = 0\n",
    "    #文本字符个数\n",
    "    length = len(text)\n",
    "    corrected_sent, detail = pycorrector.correct(text)\n",
    "    count_accu = len(detail)\n",
    "    accuracy_score.append(1-count_accu/math.log(length))\n",
    "print(accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 汇总"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "client = pymongo.MongoClient(host='localhost',port = 27017)\n",
    "db = client.spider\n",
    "collection = db.metadata_agridata\n",
    "records = collection.find()\n",
    "total = collection.count_documents\n",
    "\n",
    "complete = []\n",
    "timeline = []\n",
    "uniq = []\n",
    "consistent = [] \n",
    "accuracy_score = []\n",
    "\n",
    "# 完整性\n",
    "for item in records:\n",
    "    field_count = len(item)\n",
    "    comple_count = 0\n",
    "    for v in item.values():\n",
    "        if v != '' and v != None:\n",
    "            comple_count +=1\n",
    "    temp = comple_count/field_count\n",
    "    complete.append(temp)\n",
    "# 时效性\n",
    "    year = int(item['涉及时间'])\n",
    "    current = datetime.datetime.now().year\n",
    "    temp =1- (current-year)/50\n",
    "    timeline.append(temp)\n",
    "    # 一致性\n",
    "    consis = 9\n",
    "    Id = record['唯一标识号']\n",
    "    pattern = re.compile(\"^[\\dA-Za-z_]$\")\n",
    "    if re.match(pattern,Id) != None:\n",
    "        consis += 1\n",
    "    down = record['下载']\n",
    "    pattern = re.compile(\"[a-zA-z]+://[^\\s]*\")\n",
    "    if re.match(pattern,down) != None:\n",
    "        consis += 1\n",
    "    temp = consis/11\n",
    "    consistent.append(temp)\n",
    "    # 精确性\n",
    "#目前存在错误：摘要：采样重复数祥见监测规范。\n",
    "    text = record['标题'] + record['关键词'] + record['分类'] + record['生产者'] + record['所在机构']\n",
    "    #记录错别字的个数\n",
    "    count_accu = 0\n",
    "    #文本字符个数\n",
    "    length = len(text)\n",
    "    corrected_sent, detail = pycorrector.correct(text)\n",
    "    count_accu = len(detail)\n",
    "    accuracy_score.append(1-count_accu/math.log(length))\n",
    "    \n",
    "    \n",
    "# 唯一性\n",
    "for index in range(0,50):\n",
    "    copy1 = records.clone()\n",
    "    test = copy1[index]['标题']\n",
    "    #语料库\n",
    "    word_collection = []\n",
    "    text = \"\"\n",
    "    for record in copy1:\n",
    "        text= record['标题']\n",
    "        text_cut = jieba.cut(text)\n",
    "        temp = \"\"\n",
    "        for i in text_cut:\n",
    "            temp += i+' '\n",
    "        word_collection.append(temp)\n",
    "    del word_collection[index]\n",
    "    texts = [[word for word in document.split()] for document in word_collection] \n",
    "    # 去除无意义字符\n",
    "    stop = ['《','》','（','）','、']\n",
    "    texts = [[w for w in text if w not in  stop]for text in texts]\n",
    "    # 计算词频\n",
    "    frequency = defaultdict(int)\n",
    "    for t in texts:\n",
    "        for w in t:\n",
    "            frequency[w] += 1\n",
    "    # 建立词典\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    #对语料库进一步处理，得到新语料库\n",
    "    corpus=[dictionary.doc2bow(text)for text in texts]\n",
    "    # 将新语料库通过tf-idf model 进行处理，得到tfidf\n",
    "    tfidf=models.TfidfModel(corpus)\n",
    "    #通过token2id得到特征数\n",
    "    featurenum=len(dictionary.token2id.keys())\n",
    "    #稀疏矩阵相似度，从而建立索引\n",
    "    index=similarities.SparseMatrixSimilarity(tfidf[corpus],num_features=featurenum)\n",
    "\n",
    "    # 加载待测试文本\n",
    "    test_cut = jieba.cut(test)\n",
    "    temp2 = \"\"\n",
    "    for i in test_cut:\n",
    "        if i not in stop:\n",
    "            temp2 += i+\" \" \n",
    "    # 将测试文本通过doc2bow转化为稀疏向量\n",
    "    new_xs = dictionary.doc2bow(temp2.split())\n",
    "    #得到最终相似结果\n",
    "    sim=index[tfidf[new_xs]]\n",
    "    u = sum(sim)/len(word_collection)\n",
    "    uniq.append(1 - u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete\n",
      "\n",
      "[0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666]\n",
      "[0.7, 0.74, 0.7, 0.74, 0.74, 0.74, 0.74, 0.76, 0.76, 0.74, 0.6599999999999999, 0.6599999999999999, 0.6599999999999999, 0.6599999999999999, 0.6599999999999999, 0.6599999999999999, 0.6599999999999999, 0.6599999999999999, 0.6599999999999999, 0.6599999999999999, 0.96, 0.96, 0.96, 0.72, 0.7, 0.7, 0.7, 0.7, 0.92, 0.92, 0.76, 0.74, 0.64, 0.6799999999999999, 0.6799999999999999, 0.6799999999999999, 0.6799999999999999, 0.7, 0.76, 0.7, 0.62, 0.8200000000000001, 0.8200000000000001, 0.74, 0.8200000000000001, 0.8200000000000001, 0.74, 0.8200000000000001, 0.8200000000000001, 0.84]\n",
      "[0.964662535458195, 0.9438153571278161, 0.9507353413774042, 0.964662535458195, 0.9730938313985054, 0.9688605197648309, 0.9680108416882552, 0.9553963426519565, 0.9688434726273527, 0.986383724644096, 0.9855692496105116, 0.9855692496105116, 0.9867039116061463, 0.9508862209563352, 0.9508862209563352, 0.9508862209563352, 0.9483796345944308, 0.9508862209563352, 0.9508862209563352, 1.0, 0.9530305345352663, 0.9729787381080797, 0.969600468655402, 0.9793523563148112, 0.9793523563148112, 0.9438153571278161, 0.9519886605593623, 0.9413728220388293, 0.9817463840757098, 0.9795918367346939, 0.9525471502173768, 0.9504264772546535, 0.9504264772546535, 0.9324957605223266, 0.9416098151051877, 0.9339591700688232, 0.9416098151051877, 0.9482238368767941, 0.9482238368767941, 0.9444488941131122, 0.956552477819579, 0.9587579409886455, 0.9513168077421735, 0.9591800351532138, 0.9713678965154959, 0.9591800351532138, 0.9504264772546535, 0.9601887753043248, 0.9717029056378773, 0.9812653887522768]\n",
      "[0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182]\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "print(\"complete\"+'\\n')\n",
    "print(complete)\n",
    "print(timeline)\n",
    "print(uniq)\n",
    "print(consistent) \n",
    "print(accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
